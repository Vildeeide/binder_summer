[I 200707 13:18:41 inrepo:36] Testing notebook notebooks/02_model/mmlspark_lightgbm_criteo.ipynb
[W 200707 13:18:41 inrepo:48] No such kernel reco_pyspark, falling back on kernel language=python
[W 200707 13:18:41 inrepo:60] Using kernel python3 to provide language: python
[I 200707 13:18:42 execute:404] Executing notebook with kernel: python3
Traceback (most recent call last):
  File "/src/inrepo.py", line 112, in <module>
    main()
  File "/src/inrepo.py", line 108, in main
    test_f(opts.test, opts.output_dir)
  File "/src/inrepo.py", line 74, in run_notebook
    exported = executenb(nb, cwd=os.path.dirname(nb_path), kernel_name=kernel_name)
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 737, in executenb
    return ep.preprocess(nb, resources, km=km)[0]
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 405, in preprocess
    nb, resources = super(ExecutePreprocessor, self).preprocess(nb, resources)
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/base.py", line 69, in preprocess
    nb.cells[index], resources = self.preprocess_cell(cell, resources, index)
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 448, in preprocess_cell
    raise CellExecutionError.from_cell_and_msg(cell, out)
nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:
------------------
import os
import sys

sys.path.append("../../")

import pyspark
from pyspark.ml import PipelineModel
from pyspark.ml.feature import FeatureHasher
import papermill as pm

from reco_utils.common.spark_utils import start_or_get_spark
from reco_utils.common.notebook_utils import is_databricks
from reco_utils.dataset.criteo import load_spark_df
from reco_utils.dataset.spark_splitters import spark_random_split

# Setup MML Spark
if not is_databricks():
    # get the maven coordinates for MML Spark from databricks_install script
    from scripts.databricks_install import MMLSPARK_INFO
    packages = [MMLSPARK_INFO["maven"]["coordinates"]]
    repo = MMLSPARK_INFO["maven"].get("repo")
    spark = start_or_get_spark(packages=packages, repository=repo)
    dbutils = None
    print("MMLSpark version: {}".format(MMLSPARK_INFO['maven']['coordinates']))

from mmlspark import ComputeModelStatistics
from mmlspark import LightGBMClassifier

print("System version: {}".format(sys.version))
print("PySpark version: {}".format(pyspark.version.__version__))
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m<ipython-input-1-5a28ec44c1bc>[0m in [0;36m<module>[0;34m[0m
[1;32m      4[0m [0msys[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0;34m"../../"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;34m[0m[0m
[0;32m----> 6[0;31m [0;32mimport[0m [0mpyspark[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0;32mfrom[0m [0mpyspark[0m[0;34m.[0m[0mml[0m [0;32mimport[0m [0mPipelineModel[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m [0;32mfrom[0m [0mpyspark[0m[0;34m.[0m[0mml[0m[0;34m.[0m[0mfeature[0m [0;32mimport[0m [0mFeatureHasher[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'pyspark'
ModuleNotFoundError: No module named 'pyspark'


Container exited with status: {'Error': None, 'StatusCode': 1}
