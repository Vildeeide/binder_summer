[I 200714 13:53:29 inrepo:36] Testing notebook FINA2390/Project 5. Web Crawling/5.18/sky_tutor.ipynb
[W 200714 13:53:29 inrepo:48] No such kernel python2, falling back on kernel language=python
[W 200714 13:53:29 inrepo:60] Using kernel python3 to provide language: python
[I 200714 13:53:30 execute:404] Executing notebook with kernel: python3
Traceback (most recent call last):
  File "/src/inrepo.py", line 114, in <module>
    main()
  File "/src/inrepo.py", line 110, in main
    test_f(opts.test, opts.output_dir)
  File "/src/inrepo.py", line 75, in run_notebook
    nb, cwd=os.path.dirname(nb_path), kernel_name=kernel_name, timeout=600
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 737, in executenb
    return ep.preprocess(nb, resources, km=km)[0]
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 405, in preprocess
    nb, resources = super(ExecutePreprocessor, self).preprocess(nb, resources)
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/base.py", line 69, in preprocess
    nb.cells[index], resources = self.preprocess_cell(cell, resources, index)
  File "/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py", line 448, in preprocess_cell
    raise CellExecutionError.from_cell_and_msg(cell, out)
nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:
------------------
import csv
import pandas as pd
import os
import time

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.webdriver.common.by import By 
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

############################################################################################
#Initial variables
#This part of the code is used to set the inital parameters for the web crawler
############################################################################################
START_PAGE = 2
END_PAGE = 5
PROJECT_ABBR = "sky_tutor"
PROJECT_NAME = "Project 5"
URL = "http://www.sky-tutor.com/index2.php?page=1"

#Main window
driver =webdriver.Firefox(executable_path=r'D:\git\FINA2390-Programs\geckodriver.exe')
driver.get(URL)

#Secondary window
driver2 = webdriver.Firefox(executable_path=r'D:\git\FINA2390-Programs\geckodriver.exe')

df_columns = ['Date',
              'Year of Study',
              'Subject',
              'Sex',
              'Location',
              'Time',
              'Salary',
              'Remarks']


df_etf = pd.DataFrame(columns=df_columns)

############################################################################################
#Class
#This particular class to define the conditions for explict wait
#The condition states that the crawler will jump to next page if and only if the webpage can find the text presented
############################################################################################
class text_appear(object):
  def __init__(self, locator1, locator2, locator3):
    self.locator1 = locator1
    self.locator2 = locator2
    self.locator3 = locator3

  def __call__(self, driver):
    element1 = driver.find_element(*self.locator1) 
    element2 = driver.find_element(*self.locator2)
    element3 = driver.find_element(*self.locator3)
    if len(element1.text) != 0 and len(element2.text) !=0 and len(element3.text) !=0:
        return True
    else:
        return False

def go_to_page(page):
    if page != 1:
        print 'GOING TO PAGE' + str(page)
        driver.get('http://www.sky-tutor.com/index2.php?page='+str(page))
        print 'ARRIVED PAGE' + str(page)
    else:
        return

def save_file(state, CURRENT_PAGE):
    if state == 'backup':
        filename = '['+ PROJECT_ABBR +'][Intrim Backup][' + str(CURRENT_PAGE) + ']' + PROJECT_NAME + ' PAGE ' + str(START_PAGE) + ' to PAGE ' + str(CURRENT_PAGE) + ' Records.csv'
        df_etf.to_csv(filename, index=False,  encoding='utf_8_sig')
        print '[Intrim Backup]Saved First '+ str(CURRENT_PAGE) + 'Pages Records to CSV'
    elif state == 'all':
        filename = '['+ PROJECT_ABBR +']'+ PROJECT_NAME +' PAGE ' + str(START_PAGE) + 'to PAGE ' + str(END_PAGE) + 'Records.csv'
        df_etf.to_csv(filename, index=False, encoding='utf_8_sig')
        print 'Exported to csv'
        
    
def dig(link, row_index):
    
    #Open a new window
    driver2.get(link.get_attribute('href'))
    time.sleep(1)
    
    xsex_yof = '//*[@id="midd"]/div[2]/div[6]/div[2]/p'
    xsubject = '//*[@id="midd"]/div[2]/div[7]/div[2]/p'
    xtime = '//*[@id="midd"]/div[2]/div[5]/div[2]/p'
    xsalary = '//*[@id="midd"]/div[2]/div[8]/div[2]/p'
    xremarks = '//*[@id="midd"]/div[2]/div[9]/div[2]/p'
    
    #find fields on the new driver
    time.sleep(1)
    if len(xsubject) != 0:
        df_etf.loc[row_index, 'Subject'] = driver2.find_element_by_xpath(xsubject).text
    if len(xtime) != 0:
        df_etf.loc[row_index, 'Time']    = driver2.find_element_by_xpath(xtime).text
    if len(xsalary) != 0:    
        df_etf.loc[row_index, 'Salary']  = driver2.find_element_by_xpath(xsalary).text
    if len(xremarks) != 0:
        df_etf.loc[row_index, 'Remarks']  = driver2.find_element_by_xpath(xremarks).text
    if len(xsex_yof) !=0:
        if(len(driver2.find_element_by_xpath(xsex_yof).text) > 1):
            sex_yof = driver2.find_element_by_xpath(xsex_yof).text.split(',')
            df_etf.loc[row_index, 'Year of Study']  = sex_yof[0]
            df_etf.loc[row_index, 'Sex']  = sex_yof[1]
        else:
            df_etf.loc[row_index, 'Year of Study']  = xsex_yof.text
                                                

############################################################################################
#Main
#This part is the main function of the program
############################################################################################
def main():
    time.sleep(2)
    
    go_to_page(START_PAGE)
    time.sleep(1)
    print 'Start at PAGE'+ str(START_PAGE)
    
    n = 1
    for CURRENT_PAGE in range(START_PAGE,END_PAGE+1):
        print '--------------------PAGE ' + str(CURRENT_PAGE) +' START--------------------'
        START_TIMESTAMP = time.time()
        
        row_count = len(driver.find_elements_by_xpath('//*[@id="mid"]/div[2]/div[2]/div[2]/table'))
        
        for row in driver.find_elements_by_xpath('//*[@id="mid"]/div[2]/div[2]/div[2]/table'):
            df_etf.loc[n, 'Location']         = row.find_element_by_xpath('tbody/tr[1]/td[2]/span/a').text
            df_etf.loc[n, 'Date'] = row.find_element_by_xpath('tbody/tr[1]/td[3]/span').text

            link = row.find_element_by_xpath('tbody/tr[1]/td[2]/span/a')
            
            #open the univesity profile to get other fields
            dig(link, n)
            
            print '--------------------ROW ' + str(n)+ '/' + str(row_count) +' FINISHED--------------------'
            n = n + 1
            
        print '--------------------PAGE ' + str(CURRENT_PAGE) +' FINISH--------------------'
        END_TIMESTAMP = time.time()
        print '-TIME USED:' + str(START_TIMESTAMP-END_TIMESTAMP) + '-'
        
        #store every first 5,10,15... pages to CSV for backup purpose
        if(CURRENT_PAGE % 5) == 0:
            save_file('backup', CURRENT_PAGE)
            
        go_to_page(CURRENT_PAGE + 1)
        
    #set Source as QS for every row as this is crawl from QS
    
    #export the file to CSV
    save_file('all', CURRENT_PAGE)
    
    #close the driver
    driver.quit()
    
if __name__ == "__main__":
    main()


------------------

[0;36m  File [0;32m"<ipython-input-1-492183413735>"[0;36m, line [0;32m66[0m
[0;31m    print 'GOING TO PAGE' + str(page)[0m
[0m                        ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

SyntaxError: invalid syntax (<ipython-input-1-492183413735>, line 66)


Container exited with status: {'Error': None, 'StatusCode': 1}
